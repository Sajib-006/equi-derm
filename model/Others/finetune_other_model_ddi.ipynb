{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.metrics import (f1_score, balanced_accuracy_score, \n",
    "    classification_report, confusion_matrix, roc_curve, auc)\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python eval.py --model=DeepDerm --data_dir=DDI --eval_dir=DDI-results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Examples:\n",
    "\n",
    "# (1) w/command line interface\n",
    "# # evaluate DeepDerm on DDI and store results in `DDI-results`\n",
    "# >>>python3 eval.py --model=DeepDerm --data_dir=DDI --eval_dir=DDI-results \n",
    "\n",
    "# (2) w/python functions\n",
    "# >>>import eval\n",
    "# >>>model = eval.load_model(\"DeepDerm\") # load DeepDerm model\n",
    "# >>>eval_results = eval.eval_model(model, \"DDI\") # evaluate images in DDI folder\n",
    "\n",
    "\n",
    "# google drive paths to models\n",
    "MODEL_WEB_PATHS = {\n",
    "'HAM10000':'https://drive.google.com/uc?id=1ToT8ifJ5lcWh8Ix19ifWlMcMz9UZXcmo',\n",
    "'DeepDerm':'https://drive.google.com/uc?id=1OLt11htu9bMPgsE33vZuDiU5Xe4UqKVJ',\n",
    "# robust training algorithms\n",
    "'GroupDRO':'https://drive.google.com/uc?id=193ippDUYpMaOaEyLjd1DNsOiW0aRXL75',\n",
    "'CORAL':   'https://drive.google.com/uc?id=18rMU0nRd4LiHN9WkXoDROJ2o2sG1_GD8',\n",
    "'CDANN':   'https://drive.google.com/uc?id=1PvvgQVqcrth840bFZ3ddLdVSL7NkxiRK',\n",
    "}\n",
    "\n",
    "# thresholds determined by maximizing F1-score on the test split of the train \n",
    "#   dataset for the given algorithm\n",
    "MODEL_THRESHOLDS = {\n",
    "    'HAM10000':0.733,\n",
    "    'DeepDerm':0.687,\n",
    "    # robust training algorithms\n",
    "    'GroupDRO':0.980,\n",
    "    'CORAL':0.990,\n",
    "    'CDANN':0.980,\n",
    "}\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_dir', type=str, default=\"DDI-models\", \n",
    "        help=\"File path for where to save models.\")\n",
    "    parser.add_argument('--model', type=str, default=\"DeepDerm\", \n",
    "        help=\"Name of the model to load (HAM10000, DeepDerm, GroupDRO, CORAL,\"\\\n",
    "             \" or CDANN).\")\n",
    "    parser.add_argument('--no_download', action='store_true', default=False,\n",
    "        help=\"Set to disable downloading models.\")\n",
    "    parser.add_argument('--data_dir', type=str, default=\"DDI\", \n",
    "        help=\"Folder containing dataset to load. Structure should match the\"\\\n",
    "             \" root directory in torchvision.datasets.ImageFolder with 2\"\\\n",
    "             \" classes: benign (class 0) and malignant (class 1).\")\n",
    "    parser.add_argument('--eval_dir', type=str, default=\"DDI-results\", \n",
    "        help=\"Folder to store evaluation results.\")\n",
    "    parser.add_argument('--use_gpu', action='store_true', default=False,\n",
    "        help=\"Set to use GPU for evaluation.\")\n",
    "    parser.add_argument('--plot', action='store_true', default=False,\n",
    "        help=\"Set to show ROC plot.\")\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def load_model(model_name, save_dir=\"DDI-models\", download=True):\n",
    "    \"\"\"Load the model and download if necessary. Saves model to provided save \n",
    "    directory.\"\"\"\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model_path = os.path.join(save_dir, f\"{model_name.lower()}.pth\")\n",
    "    if not os.path.exists(model_path):\n",
    "        if not download:\n",
    "            raise Exception(\"Model not downloaded and download option not\"\\\n",
    "                            \" enabled.\")\n",
    "        else:\n",
    "            # Requires installation of gdown (pip install gdown)\n",
    "            import gdown\n",
    "            gdown.download(MODEL_WEB_PATHS[model_name], model_path)\n",
    "    model = torchvision.models.inception_v3(init_weights=False, pretrained=False, transform_input=True)\n",
    "    model.fc = torch.nn.Linear(2048, 2)\n",
    "    model.AuxLogits.fc = torch.nn.Linear(768, 2)\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model._ddi_name = model_name\n",
    "    model._ddi_threshold = MODEL_THRESHOLDS[model_name]\n",
    "    model._ddi_web_path = MODEL_WEB_PATHS[model_name]\n",
    "    return model\n",
    "\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = os.path.abspath(self.imgs[index][0])\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google drive paths to models\n",
    "MODEL_WEB_PATHS = {\n",
    "'HAM10000':'https://drive.google.com/uc?id=1ToT8ifJ5lcWh8Ix19ifWlMcMz9UZXcmo',\n",
    "'DeepDerm':'https://drive.google.com/uc?id=1OLt11htu9bMPgsE33vZuDiU5Xe4UqKVJ',\n",
    "# robust training algorithms\n",
    "'GroupDRO':'https://drive.google.com/uc?id=193ippDUYpMaOaEyLjd1DNsOiW0aRXL75',\n",
    "'CORAL':   'https://drive.google.com/uc?id=18rMU0nRd4LiHN9WkXoDROJ2o2sG1_GD8',\n",
    "'CDANN':   'https://drive.google.com/uc?id=1PvvgQVqcrth840bFZ3ddLdVSL7NkxiRK',\n",
    "}\n",
    "\n",
    "# thresholds determined by maximizing F1-score on the test split of the train \n",
    "#   dataset for the given algorithm\n",
    "MODEL_THRESHOLDS = {\n",
    "    'HAM10000':0.733,\n",
    "    'DeepDerm':0.687,\n",
    "    # robust training algorithms\n",
    "    'GroupDRO':0.980,\n",
    "    'CORAL':0.990,\n",
    "    'CDANN':0.980,\n",
    "}\n",
    "\n",
    "\n",
    "def load_model(model_name, save_dir=\"DDI-models\", download=True):\n",
    "    \"\"\"Load the model and download if necessary. Saves model to provided save \n",
    "    directory.\"\"\"\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model_path = os.path.join(save_dir, f\"{model_name.lower()}.pth\")\n",
    "    if not os.path.exists(model_path):\n",
    "        if not download:\n",
    "            raise Exception(\"Model not downloaded and download option not\"\\\n",
    "                            \" enabled.\")\n",
    "        else:\n",
    "            # Requires installation of gdown (pip install gdown)\n",
    "            import gdown\n",
    "            gdown.download(MODEL_WEB_PATHS[model_name], model_path)\n",
    "    model = torchvision.models.inception_v3(init_weights=False, pretrained=False, transform_input=True)\n",
    "    model.fc = torch.nn.Linear(2048, 2)\n",
    "    model.AuxLogits.fc = torch.nn.Linear(768, 2)\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model._ddi_name = model_name\n",
    "    model._ddi_threshold = MODEL_THRESHOLDS[model_name]\n",
    "    model._ddi_web_path = MODEL_WEB_PATHS[model_name]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = os.path.abspath(self.imgs[index][0])\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = \"finetune_other_models\"\n",
    "model_name = \"DeepDerm\" #\"HAM10000\"\n",
    "save_dir=\"finetune_other_models\"\n",
    "\n",
    "model = load_model(model_name, \n",
    "    save_dir=save_dir, download=True)\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "def train_model(model, train_dir, val_dir, epochs=100, batch_size=16, use_gpu=False, learning_rate=0.0005, show_plot=False):\n",
    "    \"\"\"Train a model on the specified dataset.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to train.\n",
    "        train_dir: Directory containing the training data.\n",
    "        val_dir: Directory containing the validation data.\n",
    "        epochs: Number of epochs to train for.\n",
    "        batch_size: Batch size for training.\n",
    "        use_gpu: Boolean to use GPU if available.\n",
    "        learning_rate: Learning rate for the optimizer.\n",
    "        show_plot: Plot training and validation loss after training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for GPU availability\n",
    "    use_gpu = use_gpu and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),             # Resize images to 256x256\n",
    "        transforms.CenterCrop(224),         # Crop a 224x224 patch from the center\n",
    "        transforms.ToTensor(),              # Convert images to Tensor\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize for pre-trained models\n",
    "        #                     std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Same normalization for consistency\n",
    "        #                     std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Setup data loaders\n",
    "    train_loader = DataLoader(\n",
    "        datasets.ImageFolder(train_dir, transform=train_transforms),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(\n",
    "        datasets.ImageFolder(val_dir, transform=val_transforms),\n",
    "        batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training and validation loop\n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        total_train_loss = 0\n",
    "        for images, labels in tqdm.tqdm(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            print(images.shape)\n",
    "            print(summary(model, input_size=(batch_size, images.shape[0], images.shape[1], images.shape[2])))\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Optionally plot the training and validation losses\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example model definition (e.g., a simple CNN or a preloaded model)\n",
    "# # model = torchvision.models.resnet18(pretrained=True)  # Just as an example\n",
    "\n",
    "# # Paths to your training and validation data\n",
    "# train_dir = 'RETFound_MAE/DDI_data/train'\n",
    "# val_dir = 'RETFound_MAE/DDI_data/val'\n",
    "\n",
    "# # Train the model\n",
    "# trained_model = train_model(model, train_dir, val_dir, epochs=100, use_gpu=True, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, image_dir, use_gpu=False, show_plot=False):\n",
    "    \"\"\"Evaluate loaded model on provided image dataset. Assumes supplied image \n",
    "    directory corresponds to `root` input for torchvision.datasets.ImageFolder\n",
    "    class. Assumes the data is split into binary/malignant labels, as this is \n",
    "    what our models are trained+evaluated on.\"\"\"\n",
    "\n",
    "    use_gpu = (use_gpu and torch.cuda.is_available())\n",
    "    device = torch.device(\"cuda\") if use_gpu else torch.device(\"cpu\")\n",
    "    dataset = ImageFolderWithPaths(\n",
    "                    image_dir,\n",
    "                    transforms.Compose([\n",
    "                        transforms.Resize(299),\n",
    "                        transforms.CenterCrop(224),\n",
    "                        transforms.ToTensor()\n",
    "                        ]))\n",
    "    print(dataset)        \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "                    dataset,\n",
    "                    batch_size=32, shuffle=False,\n",
    "                    num_workers=0, pin_memory=use_gpu)\n",
    "\n",
    "    # prepare model for evaluation\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # log output for all images in dataset\n",
    "    hat, star, all_paths = [], [], []\n",
    "    for batch in tqdm.tqdm(enumerate(dataloader)):\n",
    "        # print(\"batch:\", batch)\n",
    "        i, (images, target, paths) = batch\n",
    "        images = images.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(images)\n",
    "\n",
    "        hat.append(output[:,1].detach().cpu().numpy())\n",
    "        star.append(target.cpu().numpy())\n",
    "        all_paths.append(paths)\n",
    "\n",
    "    hat = np.concatenate(hat)\n",
    "    star = np.concatenate(star)\n",
    "    all_paths = np.concatenate(all_paths)\n",
    "    threshold = model._ddi_threshold\n",
    "    m_name = model._ddi_name\n",
    "    m_web_path = model._ddi_web_path\n",
    "\n",
    "    report = classification_report(star, (hat>threshold).astype(int), \n",
    "        target_names=[\"benign\",\"malignant\"])\n",
    "    fpr, tpr, _ = roc_curve(star, hat, pos_label=1,\n",
    "                                sample_weight=None,\n",
    "                                drop_intermediate=True)\n",
    "    auc_est = auc(fpr, tpr)\n",
    "\n",
    "    if show_plot:\n",
    "        _=plt.plot(fpr, tpr, \n",
    "            color=\"blue\", linestyle=\"-\", linewidth=2, \n",
    "            marker=\"o\", markersize=2, \n",
    "            label=f\"AUC={auc_est:.3f}\")[0]\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    eval_results = {'predicted_labels':hat, # predicted labels by model\n",
    "                    'true_labels':star,     # true labels\n",
    "                    'images':all_paths,     # image paths\n",
    "                    'report':report,        # sklearn classification report\n",
    "                    'ROC_AUC':auc_est,      # ROC-AUC\n",
    "                    'threshold':threshold,  # >= threshold ==> malignant\n",
    "                    'model':m_name,         # model name\n",
    "                    'web_path':m_web_path,  # web link to download model\n",
    "                    }\n",
    "\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolderWithPaths\n",
      "    Number of datapoints: 132\n",
      "    Root location: DDI_data/test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=299, interpolation=bilinear)\n",
      "               CenterCrop(size=(224, 224))\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:03,  1.34it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# evaluate results on data\n",
    "eval_results = eval_model(model, \"DDI_data/test\", use_gpu=True, \n",
    "            show_plot=True)\n",
    "\n",
    "# save evaluation results in a .pkl file \n",
    "if eval_dir:\n",
    "    os.makedirs(eval_dir, exist_ok=True)\n",
    "    eval_save_path = os.path.join(eval_dir, \n",
    "                                    f\"{model_name}-evaluation.pkl\")\n",
    "    with open(eval_save_path, 'wb') as f:\n",
    "        pickle.dump(eval_results, f)\n",
    "\n",
    "# print(eval_results)\n",
    "    # load results with:\n",
    "    #with open(eval_save_path, 'rb') as f:\n",
    "    #    results = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4765906362545018"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results['ROC_AUC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n      benign       0.73      0.71      0.72        98\\n   malignant       0.22      0.24      0.23        34\\n\\n    accuracy                           0.59       132\\n   macro avg       0.48      0.47      0.48       132\\nweighted avg       0.60      0.59      0.59       132\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results['report']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retfound",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
